{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and parameters\n",
    "\n",
    "data_path = '../data/300k_dataset.csv'\n",
    "data_path_train = '../data/investigation_train_large_checked.csv'\n",
    "good_model_path = '../model/good_model.onnx'\n",
    "biased_model_path = '../model/biased_model.onnx'\n",
    "features = ['persoon_leeftijd_bij_onderzoek', 'persoon_geslacht_vrouw', 'contacten_onderwerp_no_show', 'persoonlijke_eigenschappen_spreektaal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# Helper function to calculate false positive and false negative rates\n",
    "def calculate_error_rates(group_data, true_label_column, prediction_column):\n",
    "    # Calculate the confusion matrix components for each group\n",
    "    tp = np.sum((group_data[true_label_column] == 1) & (group_data[prediction_column] == 1))  # True Positive\n",
    "    tn = np.sum((group_data[true_label_column] == 0) & (group_data[prediction_column] == 0))  # True Negative\n",
    "    fp = np.sum((group_data[true_label_column] == 0) & (group_data[prediction_column] == 1))  # False Positive\n",
    "    fn = np.sum((group_data[true_label_column] == 1) & (group_data[prediction_column] == 0))  # False Negative\n",
    "\n",
    "    # Compute the error rates\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    return false_positive_rate, false_negative_rate\n",
    "\n",
    "# Helper function to calculate the predictive parity for each group\n",
    "def calculate_predictive_parity(group_data, true_label_column, prediction_column):\n",
    "    # Calculate the precision for each group where the predicted value is 1 (positive)\n",
    "    tp = np.sum((group_data[true_label_column] == 1) & (group_data[prediction_column] == 1))  # True Positive\n",
    "    fp = np.sum((group_data[true_label_column] == 0) & (group_data[prediction_column] == 1))  # False Positive\n",
    "\n",
    "    # Compute the predictive parity (precision for positive predictions)\n",
    "    predictive_parity = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    return predictive_parity\n",
    "\n",
    "# Function to perform the combinational test for Equalized Odds\n",
    "def evaluate_equalized_odds(data, predictions, features, true_label_column, fairness_threshold=0.05):\n",
    "    # Add the predictions to the original data\n",
    "    data['predictions'] = predictions\n",
    "\n",
    "    # Generate a combination column representing feature combinations\n",
    "    feature_combinations = list(itertools.product(*[data[feature].unique() for feature in features]))\n",
    "    combination_to_label = {combination: chr(65 + i) for i, combination in enumerate(feature_combinations)}\n",
    "    data['combination'] = data[features].apply(lambda row: combination_to_label[tuple(row)], axis=1)\n",
    "\n",
    "    # Calculate error rates for each group in each combination\n",
    "    error_rates = {}\n",
    "    for combination in data['combination'].unique():\n",
    "        group_data = data[data['combination'] == combination]\n",
    "        false_positive_rate, false_negative_rate = calculate_error_rates(group_data, true_label_column, 'predictions')\n",
    "        error_rates[combination] = (false_positive_rate, false_negative_rate)\n",
    "    \n",
    "    # Calculate the difference in false positive and false negative rates between groups\n",
    "    max_fp_diff = max([abs(error_rates[comb][0] - error_rates[other_comb][0]) for comb in error_rates for other_comb in error_rates])\n",
    "    max_fn_diff = max([abs(error_rates[comb][1] - error_rates[other_comb][1]) for comb in error_rates for other_comb in error_rates])\n",
    "\n",
    "    # Check if the maximum differences exceed the fairness threshold\n",
    "    fp_fair = max_fp_diff < fairness_threshold\n",
    "    fn_fair = max_fn_diff < fairness_threshold\n",
    "\n",
    "    # Return whether fairness is met for both false positive and false negative rates\n",
    "    fairness_result = fp_fair and fn_fair\n",
    "\n",
    "    return max_fp_diff, max_fn_diff, fairness_result\n",
    "\n",
    "# Function to perform the combinational test for Predictive Parity\n",
    "def evaluate_predictive_parity(data, predictions, features, true_label_column, fairness_threshold=0.05):\n",
    "    # Add the predictions to the original data\n",
    "    data['predictions'] = predictions\n",
    "\n",
    "    # Generate a combination column representing feature combinations\n",
    "    feature_combinations = list(itertools.product(*[data[feature].unique() for feature in features]))\n",
    "    combination_to_label = {combination: chr(65 + i) for i, combination in enumerate(feature_combinations)}\n",
    "    data['combination'] = data[features].apply(lambda row: combination_to_label[tuple(row)], axis=1)\n",
    "\n",
    "    # Calculate predictive parity for each group in each combination\n",
    "    predictive_parity_values = {}\n",
    "    for combination in data['combination'].unique():\n",
    "        group_data = data[data['combination'] == combination]\n",
    "        predictive_parity = calculate_predictive_parity(group_data, true_label_column, 'predictions')\n",
    "        predictive_parity_values[combination] = predictive_parity\n",
    "\n",
    "    # Calculate the difference in predictive parity between groups\n",
    "    max_parity_diff = max([abs(predictive_parity_values[comb] - predictive_parity_values[other_comb]) \n",
    "                           for comb in predictive_parity_values for other_comb in predictive_parity_values])\n",
    "\n",
    "    # Check if the maximum difference in predictive parity exceeds the fairness threshold\n",
    "    parity_fair = max_parity_diff < fairness_threshold\n",
    "\n",
    "    # Return the maximum predictive parity difference and whether the model meets fairness criteria\n",
    "    return max_parity_diff, parity_fair\n",
    "\n",
    "# Combinational test function for Equalized Odds\n",
    "def combinational_test_equalized_odds(data_path: str, model_path: str, features: list):\n",
    "\n",
    "    true_label_column = 'checked'\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_path).drop(['checked', 'Ja', 'Nee'], axis=1).astype(np.float32)\n",
    "    data_with_labels = pd.read_csv(data_path).drop(['Ja', 'Nee'], axis=1).astype(np.float32)\n",
    "\n",
    "    # Load the ONNX model\n",
    "    session = rt.InferenceSession(model_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # Run the model to get predictions for all data\n",
    "    predictions = session.run(None, {input_name: data.values.astype(np.float32)})[0]\n",
    "    print(f\"the shape of prediction is: {predictions.shape}\")\n",
    "    print(f\"the type of prediction is: {type(predictions)}\")\n",
    "\n",
    "    # Evaluate the result using the updated evaluate function for Equalized Odds\n",
    "    max_parity_diff, fairness_result = evaluate_predictive_parity(data_with_labels, predictions, features, true_label_column)\n",
    "\n",
    "    # Return the evaluation results\n",
    "    return max_parity_diff, fairness_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of prediction is: (300000,)\n",
      "the type of prediction is: <class 'numpy.ndarray'>\n",
      "maximum predictive parity difference is: 1.0\n",
      "The model does not meet the fairness criteria for Predictive Parity\n"
     ]
    }
   ],
   "source": [
    "max_parity_diff, fairness_result  = combinational_test_equalized_odds(data_path, good_model_path, features)\n",
    "print(f\"maximum predictive parity difference is: {max_parity_diff}\")\n",
    "if fairness_result:\n",
    "    print(\"The model meets the fairness criteria for Predictive Parity\")\n",
    "else:\n",
    "    print(\"The model does not meet the fairness criteria for Predictive Parity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of prediction is: (300000,)\n",
      "the type of prediction is: <class 'numpy.ndarray'>\n",
      "maximum predictive parity difference is: 1.0\n",
      "The model does not meet the fairness criteria for Predictive Parity\n"
     ]
    }
   ],
   "source": [
    "max_parity_diff, fairness_result  = combinational_test_equalized_odds(data_path, biased_model_path, features)\n",
    "print(f\"maximum predictive parity difference is: {max_parity_diff}\")\n",
    "if fairness_result:\n",
    "    print(\"The model meets the fairness criteria for Predictive Parity\")\n",
    "else:\n",
    "    print(\"The model does not meet the fairness criteria for Predictive Parity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsait4015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
